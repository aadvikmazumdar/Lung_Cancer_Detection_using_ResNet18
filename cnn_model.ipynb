{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_info():\n",
    "    \"\"\"Set up and return device information\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "    return device   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    \"\"\"Return configuration parameters\"\"\"\n",
    "    return {\n",
    "        \"IMG_SIZE\": 224,  # ResNet requires 224x224 images\n",
    "        \"BATCH_SIZE\": 32,  # Smaller batch size for HDD\n",
    "        \"EPOCHS\": 10,\n",
    "        \"DATA_DIR\": \"B:\\\\Projects\\\\lung_cancer_detection\\\\images\",\n",
    "        \"NUM_WORKERS\": 0,  # No parallel workers for HDD\n",
    "        \"LEARNING_RATE\": 0.0001,\n",
    "        \"WEIGHT_DECAY\": 1e-4,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungCancerDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, max_samples_per_class=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []  # (path, label) pairs\n",
    "        \n",
    "        # Get class names\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        print(f\"Found {len(self.classes)} classes: {self.classes}\")\n",
    "        \n",
    "        # Load paths only (no images yet)\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            files = os.listdir(class_path)\n",
    "            if max_samples_per_class:\n",
    "                files = files[:max_samples_per_class]\n",
    "            for img_name in files:\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                self.samples.append((img_path, class_idx))\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.samples)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        config = get_config()\n",
    "        \n",
    "        try:\n",
    "            # Simple image loading\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a blank tensor as fallback\n",
    "            return torch.zeros(3, config[\"IMG_SIZE\"], config[\"IMG_SIZE\"]), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders():\n",
    "    \"\"\"Prepare datasets and dataloaders with augmentation\"\"\"\n",
    "    config = get_config()\n",
    "    \n",
    "    # Enhanced augmentation for training\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(config[\"IMG_SIZE\"], scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.2)),\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(config[\"IMG_SIZE\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Get all files from the data directory\n",
    "    all_classes = sorted(os.listdir(config[\"DATA_DIR\"]))\n",
    "    all_files = []\n",
    "    for class_idx, class_name in enumerate(all_classes):\n",
    "        class_path = os.path.join(config[\"DATA_DIR\"], class_name)\n",
    "        class_files = [(os.path.join(class_path, f), class_idx) for f in os.listdir(class_path)]\n",
    "        all_files.extend(class_files)\n",
    "    \n",
    "    # Split into train and test (80/20 split)\n",
    "    import random\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(all_files)\n",
    "    \n",
    "    split_idx = int(len(all_files) * 0.8)\n",
    "    train_files = all_files[:split_idx]\n",
    "    test_files = all_files[split_idx:]\n",
    "    \n",
    "    print(f\"Total files: {len(all_files)}\")\n",
    "    print(f\"Training files: {len(train_files)}\")\n",
    "    print(f\"Test files: {len(test_files)}\")\n",
    "    \n",
    "    # Create custom datasets\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, file_list, transform=None):\n",
    "            self.file_list = file_list\n",
    "            self.transform = transform\n",
    "            self.classes = all_classes\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.file_list)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            img_path, label = self.file_list[idx]\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image, label\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "                return torch.zeros(3, config[\"IMG_SIZE\"], config[\"IMG_SIZE\"]), label\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CustomDataset(train_files, transform=train_transform)\n",
    "    test_dataset = CustomDataset(test_files, transform=test_transform)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config[\"BATCH_SIZE\"], \n",
    "        shuffle=True,\n",
    "        num_workers=config[\"NUM_WORKERS\"],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config[\"BATCH_SIZE\"], \n",
    "        shuffle=False,\n",
    "        num_workers=config[\"NUM_WORKERS\"],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_dataset, test_dataset, train_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet18_model(num_classes):\n",
    "    \"\"\"Create and return a pre-trained ResNet-18 model with customized final layer\"\"\"\n",
    "    # Load pre-trained ResNet-18\n",
    "    model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "    \n",
    "    # Freeze early layers to speed up training\n",
    "    for param in list(model.parameters())[:-2*4]:  # Freeze all but final block + FC\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace the final fully connected layer\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(dataset):\n",
    "    \"\"\"Set up and return model, criterion, optimizer, and scheduler\"\"\"\n",
    "    config = get_config()\n",
    "    num_classes = len(dataset.classes)\n",
    "    device = get_device_info()\n",
    "    \n",
    "    model = create_resnet18_model(num_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model loaded with {trainable_params:,}/{total_params:,} trainable parameters\")\n",
    "    \n",
    "    # Optimization setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=config[\"LEARNING_RATE\"],weight_decay = config[\"WEIGHT_DECAY\"])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    return model, criterion, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, scaler):\n",
    "    \"\"\"Train for one epoch and return statistics\"\"\"\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    samples_count = 0\n",
    "    \n",
    "    # Use tqdm for training progress\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Training\", unit=\"batch\")\n",
    "    \n",
    "    for inputs, labels in train_pbar:\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        # Move to device\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass with scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        samples_count += inputs.size(0)\n",
    "        \n",
    "        # Update progress bar with current loss\n",
    "        batch_time = time.time() - batch_start\n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'speed': f\"{inputs.size(0)/batch_time:.1f} img/s\"\n",
    "        })\n",
    "    \n",
    "    # Epoch statistics\n",
    "    epoch_loss = running_loss / samples_count\n",
    "    epoch_acc = running_corrects / samples_count * 100\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    return epoch_loss, epoch_acc, epoch_time\n",
    "\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    \"\"\"Validate model and return statistics\"\"\"\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "    val_samples_count = 0\n",
    "    \n",
    "    # Use tqdm for validation progress\n",
    "    val_pbar = tqdm(test_loader, desc=f\"Validation\", unit=\"batch\")\n",
    "    \n",
    "    # No gradients needed for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_running_corrects += torch.sum(preds == labels).item()\n",
    "            val_samples_count += inputs.size(0)\n",
    "            \n",
    "            # Update validation progress bar\n",
    "            val_pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Validation statistics\n",
    "    val_epoch_loss = val_running_loss / val_samples_count\n",
    "    val_epoch_acc = val_running_corrects / val_samples_count * 100\n",
    "    \n",
    "    return val_epoch_loss, val_epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_stats():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU utilization: {torch.cuda.utilization(0)}%\")\n",
    "        print(f\"GPU memory: {torch.cuda.memory_allocated(0)/1e9:.2f}GB / {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(train_losses, train_accs, val_losses, val_accs):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "        plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, train_accs, 'b-', label='Training Accuracy')\n",
    "        plt.plot(epochs, val_accs, 'r-', label='Validation Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_history.png')\n",
    "        plt.close()\n",
    "        print(\"Training history plot saved as 'training_history.png'\")\n",
    "    except ImportError:\n",
    "        print(\"Matplotlib not available. Skipping plot generation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating plots: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, device, epochs=5):\n",
    "    \"\"\"Main training loop\"\"\"\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    print(\"\\n===== Training Started =====\")\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        epoch_loss, epoch_acc, epoch_time = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, scaler\n",
    "        )\n",
    "        print(f\"Train - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%, Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Validation phase\n",
    "        val_epoch_loss, val_epoch_acc = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Validation - Loss: {val_epoch_loss:.4f}, Acc: {val_epoch_acc:.2f}%\")\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_accs.append(val_epoch_acc)\n",
    "        \n",
    "        # Update learning rate based on validation loss\n",
    "        scheduler.step(val_epoch_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            patience_counter = 0\n",
    "            print(f\"Saving best model with accuracy: {val_epoch_acc:.2f}%\")\n",
    "            torch.save(model.state_dict(), 'best_resnet18_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        # Memory cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\n===== Training Complete =====\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "===== Loading Datasets =====\n",
      "Total files: 15000\n",
      "Training files: 12000\n",
      "Test files: 3000\n",
      "\n",
      "===== Setting Up Model =====\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aadvik Mazumdar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aadvik Mazumdar\\AppData\\Local\\Temp\\ipykernel_19224\\91818013.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "c:\\Users\\Aadvik Mazumdar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 4,722,179/11,178,051 trainable parameters\n",
      "\n",
      "===== Starting Training =====\n",
      "\n",
      "===== Training Started =====\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/375 [00:00<?, ?batch/s]C:\\Users\\Aadvik Mazumdar\\AppData\\Local\\Temp\\ipykernel_19224\\2616129539.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "c:\\Users\\Aadvik Mazumdar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 375/375 [04:45<00:00,  1.31batch/s, loss=0.2914, speed=89.6 img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.2187, Acc: 90.92%, Time: 285.73s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 94/94 [00:47<00:00,  1.99batch/s, loss=0.2064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1261, Acc: 95.20%\n",
      "Saving best model with accuracy: 95.20%\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 375/375 [04:58<00:00,  1.26batch/s, loss=0.1045, speed=91.8 img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1582, Acc: 93.62%, Time: 298.14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 94/94 [00:42<00:00,  2.20batch/s, loss=0.1144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0740, Acc: 97.23%\n",
      "Saving best model with accuracy: 97.23%\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 375/375 [05:14<00:00,  1.19batch/s, loss=0.1305, speed=88.6 img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1283, Acc: 94.97%, Time: 314.96s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 94/94 [00:43<00:00,  2.15batch/s, loss=0.1410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0697, Acc: 97.33%\n",
      "Saving best model with accuracy: 97.33%\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 375/375 [05:19<00:00,  1.17batch/s, loss=0.1336, speed=86.2 img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1137, Acc: 95.42%, Time: 319.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 94/94 [00:53<00:00,  1.76batch/s, loss=0.0612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0847, Acc: 96.27%\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 375/375 [04:25<00:00,  1.41batch/s, loss=0.0365, speed=90.6 img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.1127, Acc: 95.62%, Time: 265.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 94/94 [00:41<00:00,  2.29batch/s, loss=0.0180]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0587, Acc: 97.73%\n",
      "Saving best model with accuracy: 97.73%\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 375/375 [05:14<00:00,  1.19batch/s, loss=0.0234, speed=86.7 img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0995, Acc: 96.12%, Time: 314.50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 94/94 [00:51<00:00,  1.82batch/s, loss=0.0221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0456, Acc: 98.07%\n",
      "Saving best model with accuracy: 98.07%\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 375/375 [05:47<00:00,  1.08batch/s, loss=0.0521, speed=84.3 img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0974, Acc: 96.20%, Time: 347.40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 94/94 [00:52<00:00,  1.80batch/s, loss=0.0569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0475, Acc: 98.03%\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 375/375 [05:45<00:00,  1.09batch/s, loss=0.0610, speed=79.9 img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0821, Acc: 96.88%, Time: 345.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 94/94 [00:53<00:00,  1.77batch/s, loss=0.0814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0453, Acc: 98.13%\n",
      "Saving best model with accuracy: 98.13%\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, loss=0.1401, speed=90.0 img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0857, Acc: 96.63%, Time: 258.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 94/94 [00:41<00:00,  2.24batch/s, loss=0.0232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0437, Acc: 98.50%\n",
      "Saving best model with accuracy: 98.50%\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 375/375 [05:22<00:00,  1.16batch/s, loss=0.0627, speed=89.4 img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0775, Acc: 97.02%, Time: 322.68s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 94/94 [00:51<00:00,  1.84batch/s, loss=0.1040]\n",
      "C:\\Users\\Aadvik Mazumdar\\AppData\\Local\\Temp\\ipykernel_19224\\518312727.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_resnet18_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0450, Acc: 98.40%\n",
      "\n",
      "===== Training Complete =====\n",
      "Best validation accuracy: 98.50%\n",
      "\n",
      "===== Final Evaluation =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 94/94 [00:51<00:00,  1.83batch/s, loss=0.0232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model performance - Loss: 0.0437, Accuracy: 98.50%\n",
      "\n",
      "===== Training Complete =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== MAIN FUNCTION =====\n",
    "def main():\n",
    "    \"\"\"Main function to execute the training pipeline\"\"\"\n",
    "    # Get device info\n",
    "    device = get_device_info()\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n===== Loading Datasets =====\")\n",
    "    train_dataset, test_dataset, train_loader, test_loader = get_data_loaders()\n",
    "    \n",
    "    # Setup model and training components\n",
    "    print(\"\\n===== Setting Up Model =====\")\n",
    "    model, criterion, optimizer, scheduler = setup_model(train_dataset)\n",
    "    \n",
    "    # Get configuration\n",
    "    config = get_config()\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\n===== Starting Training =====\")\n",
    "    train_model(\n",
    "        model=model, \n",
    "        train_loader=train_loader, \n",
    "        test_loader=test_loader, \n",
    "        criterion=criterion, \n",
    "        optimizer=optimizer, \n",
    "        scheduler=scheduler, \n",
    "        device=device, \n",
    "        epochs=config[\"EPOCHS\"]\n",
    "    )\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    print(\"\\n===== Final Evaluation =====\")\n",
    "    try:\n",
    "        model.load_state_dict(torch.load('best_resnet18_model.pth'))\n",
    "        final_val_loss, final_val_acc = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Final model performance - Loss: {final_val_loss:.4f}, Accuracy: {final_val_acc:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading best model: {e}\")\n",
    "    \n",
    "    print(\"\\n===== Training Complete =====\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "===== Loading Datasets =====\n",
      "Total files: 15000\n",
      "Training files: 12000\n",
      "Test files: 3000\n",
      "\n",
      "===== Setting Up Model =====\n",
      "Using device: cpu\n",
      "Model loaded with 4,722,179/11,178,051 trainable parameters\n",
      "\n",
      "===== Loading Pre-trained Model for Evaluation =====\n",
      "\n",
      "===== Performing Model Evaluation =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aadvik Mazumdar\\AppData\\Local\\Temp\\ipykernel_19224\\148678676.py:117: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- F1 Scores ---\n",
      "Micro F1 Score:     0.9850\n",
      "Macro F1 Score:     0.9848\n",
      "Weighted F1 Score:  0.9850\n",
      "Average Test Loss:  0.0436\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    lung_aca       0.99      0.96      0.98       971\n",
      "      lung_n       1.00      1.00      1.00      1023\n",
      "    lung_scc       0.97      0.99      0.98      1006\n",
      "\n",
      "    accuracy                           0.98      3000\n",
      "   macro avg       0.99      0.98      0.98      3000\n",
      "weighted avg       0.99      0.98      0.98      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import os\n",
    "\n",
    "def compute_f1_metrics(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Compute F1 metrics for model evaluation.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model\n",
    "        test_loader (torch.utils.data.DataLoader): Test data loader\n",
    "        criterion (torch.nn.Module): Loss function\n",
    "        device (torch.device): Computing device\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation metrics including F1 scores, classification report, and confusion matrix.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    class_names = test_loader.dataset.classes\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return {\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'avg_loss': total_loss / len(test_loader),\n",
    "        'class_names': class_names\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, class_names):\n",
    "    \"\"\"\n",
    "    Plot and save the confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        conf_matrix (np.ndarray): Confusion matrix\n",
    "        class_names (list): Class labels\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def perform_f1_evaluation(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Perform model evaluation using F1 metrics.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model\n",
    "        test_loader (torch.utils.data.DataLoader): Test data loader\n",
    "        criterion (torch.nn.Module): Loss function\n",
    "        device (torch.device): Computing device\n",
    "    \"\"\"\n",
    "    print(\"\\n===== Performing Model Evaluation =====\")\n",
    "    \n",
    "    eval_results = compute_f1_metrics(model, test_loader, criterion, device)\n",
    "\n",
    "    print(\"\\n--- F1 Scores ---\")\n",
    "    print(f\"Micro F1 Score:     {eval_results['f1_micro']:.4f}\")\n",
    "    print(f\"Macro F1 Score:     {eval_results['f1_macro']:.4f}\")\n",
    "    print(f\"Weighted F1 Score:  {eval_results['f1_weighted']:.4f}\")\n",
    "    print(f\"Average Test Loss:  {eval_results['avg_loss']:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(eval_results['classification_report'])\n",
    "\n",
    "    plot_confusion_matrix(eval_results['confusion_matrix'], eval_results['class_names'])\n",
    "\n",
    "    return eval_results\n",
    "\n",
    "# ===== UPDATED MAIN FUNCTION =====\n",
    "def main():\n",
    "    \"\"\"Main function to execute the evaluation pipeline\"\"\"\n",
    "    device = get_device_info()\n",
    "\n",
    "    print(\"\\n===== Loading Datasets =====\")\n",
    "    train_dataset, test_dataset, train_loader, test_loader = get_data_loaders()\n",
    "\n",
    "    print(\"\\n===== Setting Up Model =====\")\n",
    "    model, criterion, optimizer, scheduler = setup_model(train_dataset)\n",
    "\n",
    "    model_path = \"best_resnet18_model.pth\"\n",
    "\n",
    "    # Load best model if available\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"\\n===== Loading Pre-trained Model for Evaluation =====\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading best model: {e}\")\n",
    "            return\n",
    "    \n",
    "        # Perform F1 score evaluation\n",
    "        eval_results = perform_f1_evaluation(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Save evaluation results\n",
    "        torch.save(eval_results, \"evaluation_results.pth\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Error: Model file '{model_path}' not found! Please train the model first.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
